{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kd303/nvidia_llm/blob/main/Optimizing_OpenCLIP_for_Production.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the technical blueprint of our OpenCLIP production pipeline, consolidated into a single, production-ready Markdown document.\n",
        "\n",
        "---\n",
        "\n",
        "# Production Blueprint: High-Throughput OpenCLIP Inference\n",
        "\n",
        "This document outlines the architecture for serving OpenCLIP at scale using **NVIDIA Triton Inference Server**, **DALI**, and **TensorRT** on an **A100 20GB MIG slice**.\n",
        "\n",
        "## 1. System Architecture\n",
        "\n",
        "We utilize a **FastAPI Gateway** to handle incoming REST requests, translating them into high-speed **gRPC** calls to the **Triton Ensemble**.\n",
        "\n",
        "* **Gateway:** Handles HTTP/REST (for client compatibility) + converts to binary gRPC (for performance).\n",
        "* **Triton Ensemble:** \"Zero-Copy\" pipeline where DALI preprocessing and TensorRT inference occur entirely on the GPU.\n",
        "* **Infrastructure:** Deployed via Docker with A100 MIG partitioning for hardware isolation.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Model Repository Structure\n",
        "\n",
        "Your repository should follow this standard layout for the Triton Ensemble:\n",
        "\n",
        "```text\n",
        "model_repository/\n",
        "├── preprocess/\n",
        "│   ├── 1/\n",
        "│   │   └── model.py       # DALI Python pipeline\n",
        "│   └── config.pbtxt\n",
        "├── clip_vision/\n",
        "│   ├── 1/\n",
        "│   │   └── model.plan     # TensorRT engine\n",
        "│   └── config.pbtxt\n",
        "└── openclip_ensemble/\n",
        "    └── config.pbtxt       # Defines the pipeline flow\n",
        "\n",
        "```\n",
        "\n",
        "### DALI Backend Config (`preprocess/config.pbtxt`)\n",
        "\n",
        "Optimized for a 20GB MIG slice to prevent VRAM overflow.\n",
        "\n",
        "```protobuf\n",
        "name: \"preprocess\"\n",
        "backend: \"dali\"\n",
        "max_batch_size: 64\n",
        "\n",
        "instance_group [{ count: 2, kind: KIND_GPU }]\n",
        "\n",
        "```\n",
        "\n",
        "### TensorRT Vision Config (`clip_vision/config.pbtxt`)\n",
        "\n",
        "```protobuf\n",
        "name: \"clip_vision\"\n",
        "platform: \"tensorrt_plan\"\n",
        "max_batch_size: 64\n",
        "\n",
        "instance_group [{ count: 1, kind: KIND_GPU }]\n",
        "\n",
        "dynamic_batching {\n",
        "  preferred_batch_size: [ 16, 32, 64 ]\n",
        "  max_queue_delay_microseconds: 2000\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Infrastructure: Docker & MIG\n",
        "\n",
        "Use the following `docker-compose.yml` to ensure your Triton server binds correctly to the MIG device.\n",
        "\n",
        "```yaml\n",
        "services:\n",
        "  triton-backend:\n",
        "    image: nvcr.io/nvidia/tritonserver:24.01-py3\n",
        "    environment:\n",
        "      - NVIDIA_VISIBLE_DEVICES=MIG-GPU-xxxx-xxxx-xxxx  # Insert your MIG UUID\n",
        "    shm_size: '2gb' # Critical for internal tensor passing\n",
        "    volumes:\n",
        "      - ./model_repository:/models\n",
        "    command: [\"tritonserver\", \"--model-repository=/models\"]\n",
        "\n",
        "  rest-gateway:\n",
        "    build: ./api_gateway\n",
        "    ports:\n",
        "      - \"8080:8080\"\n",
        "    environment:\n",
        "      - TRITON_URL=triton-backend:8001\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Gateway Implementation (`main.py`)\n",
        "\n",
        "This Python service acts as the REST-to-gRPC bridge."
      ],
      "metadata": {
        "id": "vaFbAUU4LjWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tritonclient.grpc.aio as grpc_client\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "import numpy as np\n",
        "\n",
        "app = FastAPI()\n",
        "client = grpc_client.InferenceServerClient(url=\"triton-backend:8001\")\n",
        "\n",
        "@app.post(\"/v1/embed\")\n",
        "async def embed(file: UploadFile = File(...)):\n",
        "    # Read binary bytes\n",
        "    content = await file.read()\n",
        "    image_bytes = np.frombuffer(content, dtype=np.uint8)\n",
        "\n",
        "    # Send gRPC request to Triton Ensemble\n",
        "    inputs = [grpc_client.InferInput(\"IMAGE_BYTES\", [1, len(image_bytes)], \"UINT8\")]\n",
        "    inputs[0].set_data_from_numpy(image_bytes.reshape(1, -1))\n",
        "\n",
        "    response = await client.infer(\"openclip_ensemble\", inputs)\n",
        "    return {\"embedding\": response.as_numpy(\"EMBEDDING\").tolist()}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "utOtgdJjLjWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 5. Deployment Checklist for 20GB MIG\n",
        "\n",
        "1. **Engine Build:** Ensure your `trtexec` build command specifies the correct `--minShapes` and `--maxShapes` (e.g., `1x3x224x224` to `64x3x224x224`) to match the config.\n",
        "2. **Memory Management:** If you hit `OOM` errors during inference, lower the `max_batch_size` in your `config.pbtxt` files to `32`.\n",
        "3. **MIG Isolation:** Always verify your MIG UUID via `nvidia-smi -L` before launching containers to ensure you aren't consuming the host GPU.\n",
        "\n",
        "---\n",
        "\n",
        "[How to optimize NVIDIA TensorRT performance](https://www.youtube.com/watch?v=67ev-6Xn30U)\n",
        "\n",
        "This video provides excellent insights into profiling your TensorRT engines to ensure your model is running at peak efficiency on your specific A100 hardware."
      ],
      "metadata": {
        "id": "dlUClpBGLjWf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}